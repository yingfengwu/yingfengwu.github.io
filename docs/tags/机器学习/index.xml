<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>机器学习 - 标签 - yingfengwu</title>
        <link>https://yingfengwu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
        <description>机器学习 - 标签 - yingfengwu</description>
        <generator>Hugo -- gohugo.io</generator><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Fri, 04 Jun 2021 17:57:40 &#43;0800</lastBuildDate><atom:link href="https://yingfengwu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="self" type="application/rss+xml" /><item>
    <title>基于LightGBM的分类预测学习笔记</title>
    <link>https://yingfengwu.github.io/%E5%9F%BA%E4%BA%8Elightgbm%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/</link>
    <pubDate>Fri, 04 Jun 2021 17:57:40 &#43;0800</pubDate>
    <author>作者</author>
    <guid>https://yingfengwu.github.io/%E5%9F%BA%E4%BA%8Elightgbm%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/</guid>
    <description><![CDATA[学习知识点概要   LightGBM的介绍及应用
  基于英雄联盟数据集的LightGBM分类实战
  学习内容 LightGBM的介绍 LightGBM是2017年由微软推出的可扩展机器学习系统，是微软旗下DMKT的一个开源项目，由2014年首届 阿里巴巴大数据竞赛获胜者之一柯国霖老师带领开发。它是一款基于GBDT（梯度提升决策树）算法的分 布式梯度提升框架，为了满足缩短模型计算时间的需求，LightGBM的设计思路主要集中在减小数据对内 存与计算性能的使用，以及减少多机器并行计算时的通讯代价。
LightGBM可以看作是XGBoost的升级豪华版，在获得与XGBoost近似精度的同时，又提供了更快的训练速 度与更少的内存消耗。正如其名字中的Light所蕴含的那样，LightGBM在大规模数据集上跑起来更加优 雅轻盈，一经推出便成为各种数据竞赛中刷榜夺冠的神兵利器。
 优点：  简单易用。提供了主流的Python\C++\R语言接口，用户可以轻松使用LightGBM建模并获得相当不错的效果。 高效可扩展。在处理大规模数据集时高效迅速、高准确度，对内存等硬件资源要求不高。 鲁棒性强。相较于深度学习模型不需要精细调参便能取得近似的效果。 LightGBM直接支持缺失值与类别特征，无需对数据额外进行特殊处理   缺点：  相对于深度学习模型无法对时空位置建模，不能很好地捕获图像、语音、文本等高维数据。 在拥有海量训练数据，并能找到合适的深度学习模型时，深度学习的精度可以遥遥领先LightGBM。    LightGBM原理:
LightGBM是基于CART树的集成模型，它的思想是串联多个决策树模型共同进行决策。基模型是CART回归 树，它有两个特点：（1）CART树，是一颗二叉树。（2）回归树，最后拟合结果是连续值。
那么如何串联呢？LightGBM采用迭代预测误差的方法串联。举个通俗的例子，我们现在需要预测一辆车 价值3000元。我们构建决策树1训练后预测为2600元，我们发现有400元的误差，那么决策树2的训练目 标为400元，但决策树2的预测结果为350元，还存在50元的误差就交给第三棵树……以此类推，每一颗树 用来估计之前所有树的误差，最后所有树预测结果的求和就是最终预测结果！
XGBoost模型可以表示为以下形式，我们约定$ f_t(x) $表示前 t 颗树的和，h_t(x)表示第 t 颗决策树，模型定义如下： $$ f_t(x)=\sum_{t=1}^T h_t(x) $$
由于模型递归生成，第t步的模型由第t-1步的模型形成，则可以写成： $$ f_t(x)=f_{t-1}(x)+h_t(x) $$
LightGBM底层实现了GBDT（Gradient Boosting Decision Tree）算法，并且添加了一系列的新特性：
 基于直方图算法进行优化，使数据存储更加方便、运算更快、鲁棒性强、模型更加稳定等。 提出了带深度限制的 Leaf-wise 算法，抛弃了大多数GBDT工具使用的按层生长 (level-wise) 的决策树生长策略，而使用了带有深度限制的按叶子生长策略，可以降低误差，得到更好的精度。 提出了单边梯度采样算法，排除大部分小梯度的样本，仅用剩下的样本计算信息增益，它是一种在减少数据量和保证精度上平衡的算法。 提出了互斥特征捆绑算法，高维度的数据往往是稀疏的，这种稀疏性启发我们设计一种无损的方法来 减少特征的维度。通常被捆绑的特征都是互斥的（即特征不会同时为非零值，像one-hot），这样两个特征捆绑起来就不会丢失信息。 直接支持类别特征(Categorical Feature)，即不需要进行one-hot编码 Cache命中率优化  LightGBM重要参数 基本参数调整  num_leaves参数 这是控制树模型复杂度的主要参数，一般的我们会使num_leaves小于（2的max_depth次方）， 以防止过拟合。由于LightGBM是leaf-wise建树与XGBoost的depth-wise建树方法不同，num_leaves比depth有更大的作用。 min_data_in_leaf 这是处理过拟合问题中一个非常重要的参数.]]></description>
</item><item>
    <title>基于XGBoost的分类预测学习笔记</title>
    <link>https://yingfengwu.github.io/%E5%9F%BA%E4%BA%8Exgboost%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/</link>
    <pubDate>Wed, 02 Jun 2021 17:57:40 &#43;0800</pubDate>
    <author>作者</author>
    <guid>https://yingfengwu.github.io/%E5%9F%BA%E4%BA%8Exgboost%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/</guid>
    <description><![CDATA[学习知识点概要   XGBoost的介绍及应用
  基于天气数据集的XGBoost分类实战
  学习内容 XGBoost的介绍 XGBoost是2016年由华盛顿大学陈天奇老师带领开发的一个可扩展机器学习系统，是一个可 供用户轻松解决分类、回归或排序问题的软件包。它内部实现了梯度提升树(GBDT)模型， 并对模型中的算法进行了诸多优化，在取得高精度的同时又保持了极快的速度。
更重要的是，XGBoost在系统优化和机器学习原理方面都进行了深入的考虑。毫不夸张的讲， XGBoost提供的可扩展性，可移植性与准确性推动了机器学习计算限制的上限，该系统在单台 机器上运行速度比当时流行解决方案快十倍以上，甚至在分布式系统中可以处理十亿级的数据。
 优点：  简单易用。相对其他机器学习库，用户可以轻松使用XGBoost并获得相当不错的效果。 高效可扩展。在处理大规模数据集时速度快效果好，对内存等硬件资源要求不高。 鲁棒性强。相对于深度学习模型不需要精细调参便能取得接近的效果。 XGBoost内部实现提升树模型，可以自动处理缺失值。   缺点：  相对于深度学习模型无法对时空位置建模，不能很好地捕获图像、语音、文本等高维数据。 在拥有海量训练数据，并能找到合适的深度学习模型时，深度学习的精度可以遥遥领先XGBoost。    XGBoost原理:
XGBoost是基于CART树的集成模型，它的思想是串联多个决策树模型共同进行决策。基模型是CART回归 树，它有两个特点：（1）CART树，是一颗二叉树。（2）回归树，最后拟合结果是连续值。
那么如何串联呢？XGBoost采用迭代预测误差的方法串联。举个通俗的例子，我们现在需要预测一辆车 价值3000元。我们构建决策树1训练后预测为2600元，我们发现有400元的误差，那么决策树2的训练目 标为400元，但决策树2的预测结果为350元，还存在50元的误差就交给第三棵树……以此类推，每一颗树 用来估计之前所有树的误差，最后所有树预测结果的求和就是最终预测结果！
XGBoost模型可以表示为以下形式，我们约定$ f_t(x) $表示前 t 颗树的和，h_t(x)表示第 t 颗决策树，模型定义如下： $$ f_t(x)=\sum_{t=1}^T h_t(x) $$
由于模型递归生成，第t步的模型由第t-1步的模型形成，则可以写成： $$ f_t(x)=f_{t-1}(x)+h_t(x) $$
XGBoost底层实现了GBDT（Gradient Boosting Decision Tree）算法，并对GBDT算法做了一系列优化：
 对目标函数进行了泰勒展示的二阶展开，可以更加高效拟合误差。 提出了一种估计分裂点的算法加速CART树的构建过程，同时可以处理稀疏数据。 提出了一种树的并行策略加速迭代。 为模型的分布式算法进行了底层优化  XGBoost重要参数    eta[默认0.3]    通过为每一颗树增加权重，提高模型的鲁棒性。]]></description>
</item><item>
    <title>基于逻辑回归的分类预测学习笔记</title>
    <link>https://yingfengwu.github.io/%E5%9F%BA%E4%BA%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/</link>
    <pubDate>Tue, 01 Jun 2021 17:57:40 &#43;0800</pubDate>
    <author>作者</author>
    <guid>https://yingfengwu.github.io/%E5%9F%BA%E4%BA%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/</guid>
    <description><![CDATA[学习知识点概要   逻辑回归的介绍及应用
  基于鸢尾花数据集的分类预测实战
  学习内容 逻辑回归的介绍 逻辑回归虽名为“回归”，但实际是一种分类学习方法。
 逻辑回归（或称对数几率回归）突出的特点：模型简单和模型可解释性强 优劣势：  优点：实现简单，易于理解和实现；计算代价不高，速度很快，存储资源低； 缺点：容易欠拟合，分类精度可能不高；由于其本质上是一个线性的分类器，所以不能应对较为复杂的数据情况   对于多分类（有三个及以上输出）而言，将多个二分类的逻辑回归组合，即可实现多分类  逻辑回归原理:
通过Logistic函数（或称为Sigmoid函数），对多元线性回归方程中的变量值进行决策（分类预测）。
sigmoid函数sigmoid
"sigmoid函数
Logistic函数(本文简写为logi(z)),在z=0的时候取值为0.5，并且 logi(z) 函数的取值范围为(0,1):
$$ logi(z) = 1/(1+e^{-z}) $$
当z&gt;=0时，y&gt;=0.5，分类为1；
当z&lt;0时，y&lt;0.5，分类为0；
其对应的 y 值我们可以视为类别1的概率预测值$P$.
一般的多元线性回归方程（任意阶可导的凸函数才能作为逻辑回归的目标函数）：
$$ z = w_0 + \textstyle\sum_{i=1}^n w_i x_i $$
将回归方程代入Logistic函数，得：
$$ P = P(y=1 | x, \theta) = 1/(1+e^{w_0 + \textstyle\sum_{i=1}^n w_i x_i}) $$
则，$ P(y=1 | x, \theta) = P, P(y=0 | x, \theta) = 1 - P $， 从中学习得出系数权值w，从而得到一个针对于当前数据的特征逻辑回归模型， 对于比较重视的特征，其对应的系数权值会更大些。]]></description>
</item><item>
    <title>提升树模型</title>
    <link>https://yingfengwu.github.io/%E6%8F%90%E5%8D%87%E6%A0%91/</link>
    <pubDate>Mon, 31 May 2021 17:57:40 &#43;0800</pubDate>
    <author>作者</author>
    <guid>https://yingfengwu.github.io/%E6%8F%90%E5%8D%87%E6%A0%91/</guid>
    <description><![CDATA[回归问题的提升树方法 算法1：
输入：训练数据集$ T={(x_1,y_1),(x_2,y_2),&hellip;,(x_N,y_N)}, x_i \in X \subseteq R^n, y_i \in Y \subseteq R $
输出：提升树$ f_m(x) $
(1) 初始化$ f_0(x)=0 $
(2) 对$ m=1,2,&hellip;,M $
(2.1) 按式(1)计算残差$ r_{mi}=y_i-f_{m-1}(x_i) $
(2.2) 拟合残差$r_{mi}$学习一个回归树，得到$T(x;\Theta_m)$
(2.3) 更新$ f_m(x)=f_{m-1}(x)+T(x;\Theta_m) $
(3) 得到回归问题的提升树$ f_M(x)=\displaystyle\sum_{m=1}^M T(x;\Theta_m) $
例子：
                 $x_i$ 1 2 3 4 5 6 7 8 9   $y_i$ 5.]]></description>
</item></channel>
</rss>
