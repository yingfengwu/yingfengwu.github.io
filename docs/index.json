[{"categories":["环境搭建"],"content":"这篇文章介绍了Spark相关知识.","date":"2021-06-04","objectID":"/win10%E4%B8%8Atensorflow-cuda%E5%AE%89%E8%A3%85/","tags":["深度学习","加速训练"],"title":"win10上Tensorflow+CUDA安装","uri":"/win10%E4%B8%8Atensorflow-cuda%E5%AE%89%E8%A3%85/"},{"categories":["环境搭建"],"content":"CUDA是一种由NVIDIA推出的通用并行计算架构，该架构使GPU能够解决复杂的计算问题。 包含了CUDA指令集架构（ISA）以及GPU内部的并行计算引擎。 ","date":"2021-06-04","objectID":"/win10%E4%B8%8Atensorflow-cuda%E5%AE%89%E8%A3%85/:0:0","tags":["深度学习","加速训练"],"title":"win10上Tensorflow+CUDA安装","uri":"/win10%E4%B8%8Atensorflow-cuda%E5%AE%89%E8%A3%85/"},{"categories":["环境搭建"],"content":"CUDA安装 首先查看tensorflow官网 上测试过可行的相对应的版本: 然后，根据自己当前环境的要求，进入链接 选择相应的CUDA版本下载 安装到自己想要的路径下，然后一直下一步。 ","date":"2021-06-04","objectID":"/win10%E4%B8%8Atensorflow-cuda%E5%AE%89%E8%A3%85/:0:1","tags":["深度学习","加速训练"],"title":"win10上Tensorflow+CUDA安装","uri":"/win10%E4%B8%8Atensorflow-cuda%E5%AE%89%E8%A3%85/"},{"categories":["环境搭建"],"content":"CUDA编译 下载好之后需要用Visual Studio软件将CUDA编译生成相应的可执行文件 ","date":"2021-06-04","objectID":"/win10%E4%B8%8Atensorflow-cuda%E5%AE%89%E8%A3%85/:0:2","tags":["深度学习","加速训练"],"title":"win10上Tensorflow+CUDA安装","uri":"/win10%E4%B8%8Atensorflow-cuda%E5%AE%89%E8%A3%85/"},{"categories":["环境搭建"],"content":"tensorflow安装 用pip安装tensorflow指定版本 pip install tensorflow-gpu==1.15.4 ","date":"2021-06-04","objectID":"/win10%E4%B8%8Atensorflow-cuda%E5%AE%89%E8%A3%85/:0:3","tags":["深度学习","加速训练"],"title":"win10上Tensorflow+CUDA安装","uri":"/win10%E4%B8%8Atensorflow-cuda%E5%AE%89%E8%A3%85/"},{"categories":["环境搭建"],"content":"tensorflow用GPU进行运算测试 代码如下： import tensorflow as tf a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a') b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b') c = tf.matmul(a, b) sess = tf.Session(config=tf.ConfigProto(log_device_placement=True)) print sess.run(c) 本文简单总结，具体查看参考资料内容。 参考资料： https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html https://blog.csdn.net/ccnucb/article/details/79873460 ","date":"2021-06-04","objectID":"/win10%E4%B8%8Atensorflow-cuda%E5%AE%89%E8%A3%85/:0:4","tags":["深度学习","加速训练"],"title":"win10上Tensorflow+CUDA安装","uri":"/win10%E4%B8%8Atensorflow-cuda%E5%AE%89%E8%A3%85/"},{"categories":["天池AI训练营"],"content":"这篇文章介绍了LightGBM的分类预测.","date":"2021-06-04","objectID":"/%E5%9F%BA%E4%BA%8Elightgbm%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/","tags":["机器学习","分类","“预测”","LightGBM"],"title":"基于LightGBM的分类预测学习笔记","uri":"/%E5%9F%BA%E4%BA%8Elightgbm%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/"},{"categories":["天池AI训练营"],"content":"学习知识点概要 LightGBM的介绍及应用 基于英雄联盟数据集的LightGBM分类实战 ","date":"2021-06-04","objectID":"/%E5%9F%BA%E4%BA%8Elightgbm%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/:0:1","tags":["机器学习","分类","“预测”","LightGBM"],"title":"基于LightGBM的分类预测学习笔记","uri":"/%E5%9F%BA%E4%BA%8Elightgbm%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/"},{"categories":["天池AI训练营"],"content":"学习内容 LightGBM的介绍 LightGBM是2017年由微软推出的可扩展机器学习系统，是微软旗下DMKT的一个开源项目，由2014年首届 阿里巴巴大数据竞赛获胜者之一柯国霖老师带领开发。它是一款基于GBDT（梯度提升决策树）算法的分 布式梯度提升框架，为了满足缩短模型计算时间的需求，LightGBM的设计思路主要集中在减小数据对内 存与计算性能的使用，以及减少多机器并行计算时的通讯代价。 LightGBM可以看作是XGBoost的升级豪华版，在获得与XGBoost近似精度的同时，又提供了更快的训练速 度与更少的内存消耗。正如其名字中的Light所蕴含的那样，LightGBM在大规模数据集上跑起来更加优 雅轻盈，一经推出便成为各种数据竞赛中刷榜夺冠的神兵利器。 优点： 简单易用。提供了主流的Python\\C++\\R语言接口，用户可以轻松使用LightGBM建模并获得相当不错的效果。 高效可扩展。在处理大规模数据集时高效迅速、高准确度，对内存等硬件资源要求不高。 鲁棒性强。相较于深度学习模型不需要精细调参便能取得近似的效果。 LightGBM直接支持缺失值与类别特征，无需对数据额外进行特殊处理 缺点： 相对于深度学习模型无法对时空位置建模，不能很好地捕获图像、语音、文本等高维数据。 在拥有海量训练数据，并能找到合适的深度学习模型时，深度学习的精度可以遥遥领先LightGBM。 LightGBM原理: LightGBM是基于CART树的集成模型，它的思想是串联多个决策树模型共同进行决策。基模型是CART回归 树，它有两个特点：（1）CART树，是一颗二叉树。（2）回归树，最后拟合结果是连续值。 那么如何串联呢？LightGBM采用迭代预测误差的方法串联。举个通俗的例子，我们现在需要预测一辆车 价值3000元。我们构建决策树1训练后预测为2600元，我们发现有400元的误差，那么决策树2的训练目 标为400元，但决策树2的预测结果为350元，还存在50元的误差就交给第三棵树……以此类推，每一颗树 用来估计之前所有树的误差，最后所有树预测结果的求和就是最终预测结果！ XGBoost模型可以表示为以下形式，我们约定$ f_t(x) $表示前 t 颗树的和，h_t(x)表示第 t 颗决策树，模型定义如下： $$ f_t(x)=\\sum_{t=1}^T h_t(x) $$ 由于模型递归生成，第t步的模型由第t-1步的模型形成，则可以写成： $$ f_t(x)=f_{t-1}(x)+h_t(x) $$ LightGBM底层实现了GBDT（Gradient Boosting Decision Tree）算法，并且添加了一系列的新特性： 基于直方图算法进行优化，使数据存储更加方便、运算更快、鲁棒性强、模型更加稳定等。 提出了带深度限制的 Leaf-wise 算法，抛弃了大多数GBDT工具使用的按层生长 (level-wise) 的决策树生长策略，而使用了带有深度限制的按叶子生长策略，可以降低误差，得到更好的精度。 提出了单边梯度采样算法，排除大部分小梯度的样本，仅用剩下的样本计算信息增益，它是一种在减少数据量和保证精度上平衡的算法。 提出了互斥特征捆绑算法，高维度的数据往往是稀疏的，这种稀疏性启发我们设计一种无损的方法来 减少特征的维度。通常被捆绑的特征都是互斥的（即特征不会同时为非零值，像one-hot），这样两个特征捆绑起来就不会丢失信息。 直接支持类别特征(Categorical Feature)，即不需要进行one-hot编码 Cache命中率优化 LightGBM重要参数 基本参数调整 num_leaves参数 这是控制树模型复杂度的主要参数，一般的我们会使num_leaves小于（2的max_depth次方）， 以防止过拟合。由于LightGBM是leaf-wise建树与XGBoost的depth-wise建树方法不同，num_leaves比depth有更大的作用。 min_data_in_leaf 这是处理过拟合问题中一个非常重要的参数. 它的值取决于训练数据的样本个树和 num_leaves参 数. 将其设置的较大可以避免生成一个过深的树, 但有可能导致欠拟合. 实际应用中, 对于大数据集, 设置其为几百或几千就足够了。 max_depth 树的深度，depth 的概念在 leaf-wise 树中并没有多大作用, 因为并不存在一个从 leaves 到 depth 的合理映射。 针对训练速度的参数调整 通过设置 bagging_fraction 和 bagging_freq 参数来使用 bagging 方法。 通过设置 feature_fraction 参数来使用特征的子抽样。 选择较小的 max_bin 参数。 使用 save_binary 在未来的学习过程对数据加载进行加速。 针对准确率的参数调整 使用较大的 max_bin （学习速度可能变慢） 使用较小的 learning_rate 和较大的 num_iterations 使用较大的 num_leaves （可能导致过拟合） 使用更大的训练数据 尝试 dart 模式 针对过拟合的参数调整 使用较小的 max_bin 使用较小的 num_leaves 使用 min_data_in_leaf 和 min_sum_hessian_in_leaf 通过设置 bagging_fraction 和 bagging_freq 来使用 bagging 通过设置 feature_fraction 来使用特征子抽样 使用更大的训练数据 使用 lambda_l1, lambda_l2 和 min_gain_to_split 来使用正则 尝试 max_depth 来避免生成过深的树 LightGBM的应用 金融风控 购买行为识别 交通流量预测 环境声音分类 基因分类 生物成分分析 基于英雄联盟数据集的LightGBM分类实战 Step1: 库函数导入 Step2: 数据读取/载入 Step3: 数据信息简单查看 Step4: 可视化描述 Step5: 利用 XGBoost 进行训练与预测 Step6: 利用 XGBoost 进行特征选择 Step7: 通过调整参数获得更好的效果 英雄联盟数据集下载 代码分析如下： import numpy as np import pandas as pd from sklearn import metrics from sklearn.model_selection import train_test_split from lightgbm.sklearn import LGBMClassifier from sklearn.metrics import accuracy_score from lightgbm import plot_importance from sklearn.model_selection import GridSearchCV # 绘图函数库 import matplotlib.pyplot as plt import seaborn as sns # 我们利用Pandas自带的read_csv函数读取并转化为DataFrame格式 df = pd.read_csv('../Data/LOL/high_diamond_ranked_10min.csv') # 利用.info()查看数据的整体信息 print(df.info()) # 进行简单的数据查看，我们可以利用.head()头部.tail()尾部 print(df.head()) print(df.tail()) # 标注标签并利用value_counts函数查看训练集标签的数量 y = df.blueWins print(y.value_counts()) # 标注特征列，drop_cols中存放非特征列，然后丢弃 drop_cols = ['gameId', 'blueWins'] x = df.drop(drop_cols, axis=1) # 对于特征进行一些统计描述 print(x.describe()) # 根据上面的描述，我们可以去除一些重复变量，比如只要知道蓝队是否拿到一血， # 我们就知道红队有没有拿到，可以去除红队的相关冗余数据。 drop_cols = ['redFirstBlood','redKills','redDeaths', 'redGoldDiff','redExperienceDiff', 'blueCSPerMin', 'blueGoldPerMin','redCSPerMin','redGoldPerMin'] x.drop(drop_cols, axis=1, inplace=True) # 减去平均数除以标准差相当于对原始数据进行了线性变换，没有改变数据之间的相 # 对位置，也没有改变数据的分布，只是数据的平均数变成0，标准差变成1。根本不 # 会变成正态分布，除非它本来就是。 data = x data_std = (data - data.mean()) / data.std() data = pd.concat([y, data_std.iloc[:, 0:9]], axis=1) data = pd.melt(data, id_vars='blueWins', var_name='Features', value_name='Values') fig, ax = plt.subplots(1, 2, figsize=(15, 5)) # 绘制小提琴图","date":"2021-06-04","objectID":"/%E5%9F%BA%E4%BA%8Elightgbm%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/:0:2","tags":["机器学习","分类","“预测”","LightGBM"],"title":"基于LightGBM的分类预测学习笔记","uri":"/%E5%9F%BA%E4%BA%8Elightgbm%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/"},{"categories":["天池AI训练营"],"content":"学习问题与解答 LightGBM是怎么做到训练速度更快且内存消耗更少得呢？ 对于每个特征，需要扫描所有得数据实例去估计所有可能划分的点的信息增益是很耗时的，为了解决该问题， 微软 相关 人员提出了两个新的技术：基于梯度的单边采样和独有的特征绑定。用基于梯度的单边采样的方法，可以用 小梯度排除大多重要的数据实例，并且用剩下的实例去估计信息增益。该文证实了基于梯度的单边采样的方法 用小规模的数据即可获得相当准确的信息增益估计。该文用独有的特征绑定去相互地绑定独有特征来减少特征 数量，而且找到最优绑定是NP难问题，但是贪婪算法可以实现很好的近似率。 排他特征绑定: 在稀疏特征空间，许多特征是相互排斥地，也就是它们不会同时为非零值（有些特征必为零，有些不为零）。 直方图构建的复杂度从O(data,feature)到O(data,bundle)，且bundle值远小于feature值。这里有两个问题 需要解决，一个是哪些特征需要被绑定在一起，另一个是怎样去构建绑定。 定理1：划分特征到小批量的排他特征的问题是NP难的。 证明：该文将图着色问题归约到该问题中。因为图着色问题是NP难的，所以可以由此推出我们的结论。 ","date":"2021-06-04","objectID":"/%E5%9F%BA%E4%BA%8Elightgbm%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/:0:3","tags":["机器学习","分类","“预测”","LightGBM"],"title":"基于LightGBM的分类预测学习笔记","uri":"/%E5%9F%BA%E4%BA%8Elightgbm%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/"},{"categories":["天池AI训练营"],"content":"学习思考与总结 通过此次得学习，我学到了LightGBM的基本原理及其相关应用，LightGBM训练速度更快且内存消耗更少。 ","date":"2021-06-04","objectID":"/%E5%9F%BA%E4%BA%8Elightgbm%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/:0:4","tags":["机器学习","分类","“预测”","LightGBM"],"title":"基于LightGBM的分类预测学习笔记","uri":"/%E5%9F%BA%E4%BA%8Elightgbm%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/"},{"categories":["天池AI训练营"],"content":"这篇文章介绍了XGBoost的分类预测.","date":"2021-06-02","objectID":"/%E5%9F%BA%E4%BA%8Exgboost%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/","tags":["机器学习","分类","“预测”","XGBoost"],"title":"基于XGBoost的分类预测学习笔记","uri":"/%E5%9F%BA%E4%BA%8Exgboost%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/"},{"categories":["天池AI训练营"],"content":"学习知识点概要 XGBoost的介绍及应用 基于天气数据集的XGBoost分类实战 ","date":"2021-06-02","objectID":"/%E5%9F%BA%E4%BA%8Exgboost%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/:0:1","tags":["机器学习","分类","“预测”","XGBoost"],"title":"基于XGBoost的分类预测学习笔记","uri":"/%E5%9F%BA%E4%BA%8Exgboost%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/"},{"categories":["天池AI训练营"],"content":"学习内容 XGBoost的介绍 XGBoost是2016年由华盛顿大学陈天奇老师带领开发的一个可扩展机器学习系统，是一个可 供用户轻松解决分类、回归或排序问题的软件包。它内部实现了梯度提升树(GBDT)模型， 并对模型中的算法进行了诸多优化，在取得高精度的同时又保持了极快的速度。 更重要的是，XGBoost在系统优化和机器学习原理方面都进行了深入的考虑。毫不夸张的讲， XGBoost提供的可扩展性，可移植性与准确性推动了机器学习计算限制的上限，该系统在单台 机器上运行速度比当时流行解决方案快十倍以上，甚至在分布式系统中可以处理十亿级的数据。 优点： 简单易用。相对其他机器学习库，用户可以轻松使用XGBoost并获得相当不错的效果。 高效可扩展。在处理大规模数据集时速度快效果好，对内存等硬件资源要求不高。 鲁棒性强。相对于深度学习模型不需要精细调参便能取得接近的效果。 XGBoost内部实现提升树模型，可以自动处理缺失值。 缺点： 相对于深度学习模型无法对时空位置建模，不能很好地捕获图像、语音、文本等高维数据。 在拥有海量训练数据，并能找到合适的深度学习模型时，深度学习的精度可以遥遥领先XGBoost。 XGBoost原理: XGBoost是基于CART树的集成模型，它的思想是串联多个决策树模型共同进行决策。基模型是CART回归 树，它有两个特点：（1）CART树，是一颗二叉树。（2）回归树，最后拟合结果是连续值。 那么如何串联呢？XGBoost采用迭代预测误差的方法串联。举个通俗的例子，我们现在需要预测一辆车 价值3000元。我们构建决策树1训练后预测为2600元，我们发现有400元的误差，那么决策树2的训练目 标为400元，但决策树2的预测结果为350元，还存在50元的误差就交给第三棵树……以此类推，每一颗树 用来估计之前所有树的误差，最后所有树预测结果的求和就是最终预测结果！ XGBoost模型可以表示为以下形式，我们约定$ f_t(x) $表示前 t 颗树的和，h_t(x)表示第 t 颗决策树，模型定义如下： $$ f_t(x)=\\sum_{t=1}^T h_t(x) $$ 由于模型递归生成，第t步的模型由第t-1步的模型形成，则可以写成： $$ f_t(x)=f_{t-1}(x)+h_t(x) $$ XGBoost底层实现了GBDT（Gradient Boosting Decision Tree）算法，并对GBDT算法做了一系列优化： 对目标函数进行了泰勒展示的二阶展开，可以更加高效拟合误差。 提出了一种估计分裂点的算法加速CART树的构建过程，同时可以处理稀疏数据。 提出了一种树的并行策略加速迭代。 为模型的分布式算法进行了底层优化 XGBoost重要参数 eta[默认0.3] 通过为每一颗树增加权重，提高模型的鲁棒性。 典型值为0.01-0.2。 min_child_weight[默认1] 决定最小叶子节点样本权重和。 这个参数可以避免过拟合。当它的值较大时，可以避免模型学习到局部的特殊样本。 但是如果这个值过高，则会导致模型拟合不充分。 max_depth[默认6] 这个值也是用来避免过拟合的。max_depth越大，模型会学到更具体更局部的样本。 典型值：3-10 max_leaf_nodes 树上最大的节点或叶子的数量。 可以替代max_depth的作用。 这个参数的定义会导致忽略max_depth参数。 gamma[默认0] 在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了节点分裂所需的 最小损失函数下降值。 这个参数的值越大，算法越保守。这个参数的值和损失函数息息相关。 max_delta_step[默认0] 这参数限制每棵树权重改变的最大步长。如果这个参数的值为0，那就意味着没有约束。如果它被赋予了某个正值，那么它会让这个算法更加保守。 但是当各类别的样本十分不平衡时，它对分类问题是很有帮助的。 subsample[默认1] 这个参数控制对于每棵树，随机采样的比例。 减小这个参数的值，算法会更加保守，避免过拟合。但是，如果这个值设置得过小，它可能会导致欠拟合。 典型值：0.5-1 colsample_bytree[默认1] 用来控制每棵随机采样的列数的占比(每一列是一个特征)。 典型值：0.5-1 colsample_bylevel[默认1] 用来控制树的每一级的每一次分裂，对列数的采样的占比。 subsample参数和colsample_bytree参数可以起到相同的作用，一般用不到。 lambda[默认1] 权重的L2正则化项。(和Ridge regression类似)。 这个参数是用来控制XGBoost的正则化部分的。虽然大部分数据科学家很少用到这个参数，但是这个参数在减少过拟合上还是可以挖掘出更多用处的。 alpha[默认1] 权重的L1正则化项。(和Lasso regression类似)。 可以应用在很高维度的情况下，使得算法的速度更快。 scale_pos_weight[默认1] 在各类别样本十分不平衡时，把这个参数设定为一个正值，可以使算法更快收敛。 XGBoost的应用 商店销售额预测 高能物理事件分类 web文本分类 用户行为预测 运动检测 广告点击率预测 恶意软件分类 灾害风险预测 线课程退学率预测 基于天气数据集的XGBoost分类实战 Step1: 库函数导入 Step2: 数据读取/载入 Step3: 数据信息简单查看 Step4: 可视化描述 Step5: 对离散变量进行编码 Step6: 利用 XGBoost 进行训练与预测 Step7: 利用 XGBoost 进行特征选择 Step8: 通过调整参数获得更好的效果 天气数据集下载 代码分析如下： import numpy as np import pandas as pd # 绘图函数库 import seaborn as sns # 为了正确评估模型性能，将数据划分为训练集和测试集，并在训练集上训练模型，在测试集上验证模型性能。 from sklearn.model_selection import train_test_split # 导入XGBoost模型 from xgboost.sklearn import XGBClassifier from sklearn.metrics import accuracy_score from xgboost import plot_importance # 从sklearn库中导入网格调参函数 from sklearn.model_selection import GridSearchCV from sklearn import metrics import matplotlib from matplotlib import pyplot as plt # 我们利用Pandas自带的read_csv函数读取并转化为DataFrame格式 data = pd.read_csv(\"../Data/weather/train.csv\") # 利用.info()查看数据的整体信息 print(data.info()) # 用-1填补NaN值 data = data.fillna(-1) # 利用value_counts函数查看训练集标签的数量 pd.Series(data['RainTomorrow']).value_counts() # 对于特征进行一些统计描述, 均值、标准差、最小值等 print(data.describe()) # 获得每一列的数据是浮点数的列名 numerical_features = [x for x in data.columns if data[x].dtype == np.float] # 获得每一列的数据不是浮点数且不是RainTomorrow的列名，RainTomorrow是明天是否下雨的布尔值，即标签值 category_features = [x for x in data.columns if data[x].dtype != np.float and x != 'RainTomorrow'] # 选取三个特征与标签组合的散点可视化， # 三个参数为：数据、画图的类型选择直方图、色彩（以RainTomorrow值的种类为颜色种类进行画图） sns.pairplot(data=data[['Rainfall', 'Evaporation', 'Sunshine'] + ['RainTomorrow']], diag_kind='hist', hue='RainTomorrow') # 弹出画好的图 plt.show() # 获取每一列的数据是浮点数的列名 for col in data[numerical_features].columns: if col != 'RainTomorrow': # 画箱型图，x轴为RainTomorrow值，y轴为col值，saturation为色彩饱和度 sns.boxplot(x='RainTomorrow', y=col, saturation=0.5, palette='pastel', data=data) plt.title(col) # 设置图上方的标题 plt.show() tlog = {} for i in category_features: # 利用value_counts函数查看在训练集标签为yes中， # 列表category_features里的每个列名下数据的数量个数 ","date":"2021-06-02","objectID":"/%E5%9F%BA%E4%BA%8Exgboost%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/:0:2","tags":["机器学习","分类","“预测”","XGBoost"],"title":"基于XGBoost的分类预测学习笔记","uri":"/%E5%9F%BA%E4%BA%8Exgboost%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/"},{"categories":["天池AI训练营"],"content":"学习问题与解答 XGBoost与LightGBM进行比较，有什么不同？ 答：获得的精度是差不多的，几乎一样，但是，LightGBM比XGBoost训练得更快且内存消耗更少。 XGBoost底层的树结构怎么运作？ ","date":"2021-06-02","objectID":"/%E5%9F%BA%E4%BA%8Exgboost%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/:0:3","tags":["机器学习","分类","“预测”","XGBoost"],"title":"基于XGBoost的分类预测学习笔记","uri":"/%E5%9F%BA%E4%BA%8Exgboost%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/"},{"categories":["天池AI训练营"],"content":"学习思考与总结 通过此次得学习，我学到了XGBoost的基本原理及其相关应用。但是仍然有其局限性，训练速度较慢，内存消耗较大。 ","date":"2021-06-02","objectID":"/%E5%9F%BA%E4%BA%8Exgboost%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/:0:4","tags":["机器学习","分类","“预测”","XGBoost"],"title":"基于XGBoost的分类预测学习笔记","uri":"/%E5%9F%BA%E4%BA%8Exgboost%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/"},{"categories":["LeetCode"],"content":"这篇文章展示了动态规划的总结.","date":"2021-06-01","objectID":"/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%80%BB%E7%BB%93/","tags":["动态规划"],"title":"动态规划总结（多示例+讲解）","uri":"/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%80%BB%E7%BB%93/"},{"categories":["LeetCode"],"content":"动态规划功能强大，它能够解决子问题并使用这些答案来解决大问题。但 仅当每个子问题都是离散的，即不依赖于其他子问题时，动态规划才管用。 比如，想去以下地方旅游4天，假设将埃菲尔铁塔加入“背包”后，卢浮宫将 更“便宜”：只要1天时间，而不是1.5天。用动态规划对这种情况建模呢？ 这是没办法建模的，因为存在依赖关系。 景点 停留天数 评分 埃菲尔铁塔 1.5天 8 卢浮宫 1.5天 9 巴黎圣母院 1.5天 7 ","date":"2021-06-01","objectID":"/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%80%BB%E7%BB%93/:0:0","tags":["动态规划"],"title":"动态规划总结（多示例+讲解）","uri":"/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%80%BB%E7%BB%93/"},{"categories":["LeetCode"],"content":"一、斐波那契数列求解 题目: 写一个函数，输入 n ，求斐波那契（Fibonacci）数列的第 n 项（即 F(N)）。斐波那契数列的定义如下： F(0) = 0, F(1) = 1 F(N) = F(N - 1) + F(N - 2), 其中 N \u003e 1. 斐波那契数列由 0 和 1 开始，之后的斐波那契数就是由之前的两数相加而得出。 😂 递归法 原理： 把f(n)问题的计算拆分成 f(n-1)和f(n−2)两个子问题的计算，并递归，以f(0)和f(1)为终止条件。 缺点： 大量重复的递归计算，例如f(n)和f(n - 1)两者向下递归需要 各自计算f(n−2)的值。 class Solution(object): # 该算法n越大执行越慢 def fib(self, n): if n == 0: return 0 elif n == 1: return 1 else return self.fib(n-1) + self.fib(n-2) 记忆化递归法 原理： 在递归法的基础上，新建一个长度为 n 的数组，用于在递归时存储 f(0) 至 f(n) 的 数字值，重复遇到某数字则直接从数组取用，避免了重复的递归计算。 缺点： 记忆化存储需要使用 O(N) 的额外空间。 class Solution(object): def fib(self, n): f = [0]*100 # 定义一定长度的数组 if n == 1 or n == 0: return n elif f[n] != 0: return f[n] else: f[n] = self.fib(n-1) + self.fib(n-2) # 保存至数组f避免重复计算 return f[n] 动态规划法 原理： 以斐波那契数列性质 f(n + 1) = f(n) + f(n - 1)为转移方程。从计算效率、空间复杂度上看，动态规划是本题的最佳解法。 状态定义： 设 dp 为一维数组，其中 dp[i] 的值代表 斐波那契数列第 i 个数字 。 转移方程： dp[i + 1] = dp[i] + dp[i - 1] ，即对应数列定义 f(n + 1) = f(n) + f(n - 1)； 初始状态： dp[0] = 0, dp[1]=1 ，即初始化前两个数字； 返回值： dp[n]，即斐波那契数列的第 n 个数字。 class Solution(object): def fib(self, n): a, b = 0, 1 for _ in range(n): a, b = b, a + b return a ","date":"2021-06-01","objectID":"/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%80%BB%E7%BB%93/:0:1","tags":["动态规划"],"title":"动态规划总结（多示例+讲解）","uri":"/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%80%BB%E7%BB%93/"},{"categories":["LeetCode"],"content":"二、把数字翻译成字符串 题目： 给定一个数字，我们按照如下规则把它翻译为字符串：0 翻译成 “a” ，1 翻译成 “b”，……，11 翻译成 “l”，……，25 翻译成 “z”。一个数字可能有 多个翻译。请编程实现一个函数，用来计算一个数字有多少种不同的翻译方法。 示例1: 输入: 12258 输出: 5 解释: 12258有5种不同的翻译，分别是\"bccfi\", “bwfi”, “bczi”, “mcfi\"和\"mzi” 动态规划法 状态定义： 状态定义： 设动态规划列表 dp，dp[i] 代表以 $x_i$ 为结尾的数字的翻译方案数量 转移方程： 若 $x_i$和$x_{i-1}$组成的两位数字可被整体翻译，则 dp[i] = dp[i - 1] + dp[i - 2]，否则 dp[i] = dp[i - 1]。 $$ dp[i] = \\begin{cases} dp[i-1]+dp[i-2], \u0026 (10x_{i-1} + x_{i-1})\\in[10，25] \\\\ dp[i-1], \u0026 (10x_{i-1} + x_i)\\in[0,10)\\bigcup(25,99] \\end{cases} $$ 初始状态： dp[0] = dp[1] = 1，即 “无数字” 和 “第1位数字” 的翻译方法数量均为 1； 返回值： dp[n]，即此数字的翻译方案数量； 第一种写法： class Solution(object): def translateNum(self, num): s = str(num) # 当 num 第 1, 2位的组成的数字 ∈ [10,25]时， # 显然应有 2 种翻译方法，即 dp[2] = dp[1] + dp[0] = 2， # 而显然 dp[1] = 1，因此推出 dp[0] = 1，即初始化a=b=1 a = b = 1 for i in range(2, len(s) + 1): tmp = s[i - 2:i] if \"10\" \u003c= tmp \u003c= \"25\": c = a + b else: c = a b = a a = c return a 第二种写法： class Solution(object): def translateNum(self, num): s = str(num) a = b = 1 for i in range(2, len(s) + 1): a, b = (a + b if \"10\" \u003c= s[i - 2:i] \u003c= \"25\" else a), a return a ","date":"2021-06-01","objectID":"/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%80%BB%E7%BB%93/:0:2","tags":["动态规划"],"title":"动态规划总结（多示例+讲解）","uri":"/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%80%BB%E7%BB%93/"},{"categories":["LeetCode"],"content":"三、青蛙跳台阶问题求解 题目： 一只青蛙一次可以跳上1级台阶，也可以跳上2级台阶。求该青蛙跳上一个 n 级的台阶总共有多少种跳法。 答案需要取模 1e9+7（1000000007），如计算初始结果为：1000000008，请返回 1。 动态规划法 状态定义： 设 dp 为一维数组，其中 dp[i] 的值代表斐波那契数列的第 i 个数字。 转移方程： dp[i + 1] = dp[i] + dp[i - 1]，即对应数列定义 f(n + 1) = f(n) + f(n - 1)； 初始状态： dp[0] = 1, dp[1]=1 ，即初始化前两个数字； 返回值： dp[n]，即斐波那契数列的第 n 个数字。 class Solution(object): def numWays(self, n): if n \u003c 2: return 1 d = [1]*(n+1) for i in range(2, n+1): d[i] = d[i-1] + d[i-2] return d[-1] % 1000000007 ","date":"2021-06-01","objectID":"/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%80%BB%E7%BB%93/:0:3","tags":["动态规划"],"title":"动态规划总结（多示例+讲解）","uri":"/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%80%BB%E7%BB%93/"},{"categories":["LeetCode"],"content":"四、连续子数组的最大和 题目： 输入一个整型数组，数组中的一个或连续多个整数组成一个子数组。求所有子数组的和的最大值。 要求时间复杂度为O(n)。 示例1: 输入: nums = [-2,1,-3,4,-1,2,1,-5,4] 输出: 6 解释: 连续子数组 [4,-1,2,1] 的和最大，为 6。 动态规划法 原理： 以斐波那契数列性质 f(n + 1) = f(n) + f(n - 1)为转移方程。从计算效率、空间复杂度上看，动态规划是本题的最佳解法。 状态定义： 设 dp 为一维数组，其中 dp[i] 的值代表 斐波那契数列第 i 个数字 。 转移方程： dp[i + 1] = dp[i] + dp[i - 1]，即对应数列定义f(n+1)=f(n)+f(n−1) ； 初始状态： dp[0] = 0, dp[1]=1，即初始化前两个数字； 返回值： dp[n] ，即斐波那契数列的第 n 个数字。 class Solution(object): def maxSubArray(self, nums): \"\"\" :type nums: List[int] :rtype: int \"\"\" for i in range(1, len(nums)): nums[i] += max(nums[i-1], 0) return max(nums) class Solution { public: int maxSubArray(vector\u003cint\u003e\u0026 nums) { int max_num = nums[0]; for(int i=1;i\u003cnums.size();i++){ nums[i] += max(nums[i-1], 0); if(nums[i] \u003e max_num){ max_num = nums[i]; } } return max_num; } }; ","date":"2021-06-01","objectID":"/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%80%BB%E7%BB%93/:0:4","tags":["动态规划"],"title":"动态规划总结（多示例+讲解）","uri":"/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%80%BB%E7%BB%93/"},{"categories":["LeetCode"],"content":"五、放苹果 题目： 把m个同样的苹果放在n个同样的盘子里，允许有的盘子空着不放，问共有多少种不同的分法？（用K表示）5，1，1和1，5，1 是同一种分法。 数据范围：0\u003c=m\u003c=10，1\u003c=n\u003c=10。 示例1: 输入： 7 3 输出： 8 动态规划法 原理： 递推的方式，利用公式来填表。将m个苹果放入n个盘子里，包含了2个事件：至少有一个盘子空着的事件A， 和所有盘子都不空的事件B（每个盘子都至少有一个苹果）。A∪B即所有情况。A就是求f(m, n-1)，B就是f(m-n, n)。 事件B表示每个盘子都有一个苹果时再放m-n个苹果，等价于每个盘子都没有苹果时放m-n个苹果，所以可以直接写成 f(m-n, n)。注意m-n可能为负数，此时要返回0。例如，f(4,4)=f(4,3)+f(0,4)，f(0,4)等于1，表示在4个盘子中各放1个苹果。 状态定义： 设 f[m][n] 为二维数组。 转移方程： f(m, n)=f(m, n-1)+f(m-n, n)； 初始状态： f[0][1] = 0, f[1][1]=1，即初始化前两个数字, 注意数据范围； 返回值： f[m][n]。 while True: try: m,n = map(int, input().split()) # 获取输入数据 if (m \u003e= 0) \u0026 (m \u003c= 10) \u0026 (n \u003e=1) \u0026 (n \u003c= 10): c = [[0 for _ in range(n+1)] for _ in range(m+1)] for i in range(m+1): for j in range(1, n+1): if i \u003c= 1 or j == 1: c[i][j] = 1 elif j \u003e i: c[i][j] = c[i][j-1] elif j \u003c= i: c[i][j] = c[i][j-1] + c[i-j][j] print(c[-1][-1]) except: break 递归法 def func(m, n): if m \u003c= 0 or n == 1: return 1 if n \u003e m: return func(m, n-1) return func(m, n-1) + func(m-n, n) while True: try: m,n = map(int, input().split()) # 获取输入数据 if (m \u003e= 0) \u0026 (m \u003c= 10) \u0026 (n \u003e=1) \u0026 (n \u003c= 10): print(func(m, n)) except: break 总结： 由放苹果的示例可知，递归法和动态规划法相似，但是，当数据量较大的时候，递归法会进行大量的重复计算，效率降低。 此时就需要使用动态规划法。 总的来看，使用的公式是一样的，递归法使用函数存储和计算数据，而动态规划法是使用数组存储和 计算数据。 ","date":"2021-06-01","objectID":"/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%80%BB%E7%BB%93/:0:5","tags":["动态规划"],"title":"动态规划总结（多示例+讲解）","uri":"/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%80%BB%E7%BB%93/"},{"categories":["天池AI训练营"],"content":"这篇文章介绍了逻辑回归的分类预测.","date":"2021-06-01","objectID":"/%E5%9F%BA%E4%BA%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/","tags":["机器学习","逻辑回归","分类","“预测”"],"title":"基于逻辑回归的分类预测学习笔记","uri":"/%E5%9F%BA%E4%BA%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/"},{"categories":["天池AI训练营"],"content":"学习知识点概要 逻辑回归的介绍及应用 基于鸢尾花数据集的分类预测实战 ","date":"2021-06-01","objectID":"/%E5%9F%BA%E4%BA%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/:0:1","tags":["机器学习","逻辑回归","分类","“预测”"],"title":"基于逻辑回归的分类预测学习笔记","uri":"/%E5%9F%BA%E4%BA%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/"},{"categories":["天池AI训练营"],"content":"学习内容 逻辑回归的介绍 逻辑回归虽名为“回归”，但实际是一种分类学习方法。 逻辑回归（或称对数几率回归）突出的特点：模型简单和模型可解释性强 优劣势： 优点：实现简单，易于理解和实现；计算代价不高，速度很快，存储资源低； 缺点：容易欠拟合，分类精度可能不高；由于其本质上是一个线性的分类器，所以不能应对较为复杂的数据情况 对于多分类（有三个及以上输出）而言，将多个二分类的逻辑回归组合，即可实现多分类 逻辑回归原理: 通过Logistic函数（或称为Sigmoid函数），对多元线性回归方程中的变量值进行决策（分类预测）。 sigmoid函数sigmoid \"\rsigmoid函数\r Logistic函数(本文简写为logi(z)),在z=0的时候取值为0.5，并且 logi(z) 函数的取值范围为(0,1): $$ logi(z) = 1/(1+e^{-z}) $$ 当z\u003e=0时，y\u003e=0.5，分类为1； 当z\u003c0时，y\u003c0.5，分类为0； 其对应的 y 值我们可以视为类别1的概率预测值$P$. 一般的多元线性回归方程（任意阶可导的凸函数才能作为逻辑回归的目标函数）： $$ z = w_0 + \\textstyle\\sum_{i=1}^n w_i x_i $$ 将回归方程代入Logistic函数，得： $$ P = P(y=1 | x, \\theta) = 1/(1+e^{w_0 + \\textstyle\\sum_{i=1}^n w_i x_i}) $$ 则，$ P(y=1 | x, \\theta) = P, P(y=0 | x, \\theta) = 1 - P $， 从中学习得出系数权值w，从而得到一个针对于当前数据的特征逻辑回归模型， 对于比较重视的特征，其对应的系数权值会更大些。 逻辑回归的应用 预测受伤患者的死亡率 基于观察到的患者特征（年龄，性别，体重指数,各种血液检查的结果等）分析预测发生特定疾病（例如糖尿病，冠心病）的风险 预测在给定的过程中，系统或产品的故障的可能性 预测客户购买产品或中止订购的倾向 预测一个人选择进入劳动力市场的可能性 预测房主拖欠抵押贷款的可能性 基于鸢尾花数据集的分类预测实战 代码链接： ","date":"2021-06-01","objectID":"/%E5%9F%BA%E4%BA%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/:0:2","tags":["机器学习","逻辑回归","分类","“预测”"],"title":"基于逻辑回归的分类预测学习笔记","uri":"/%E5%9F%BA%E4%BA%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/"},{"categories":["天池AI训练营"],"content":"学习问题与解答 在多分类问题中，如何将多个二分类的逻辑回归进行组合以实现多分类 答：多分类会得到多个方程，然后将学到的系数权值$w$代入方程中，查看概率值$P$ RDDs是Spark分发数据和计算的基础抽象类 ","date":"2021-06-01","objectID":"/%E5%9F%BA%E4%BA%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/:0:3","tags":["机器学习","逻辑回归","分类","“预测”"],"title":"基于逻辑回归的分类预测学习笔记","uri":"/%E5%9F%BA%E4%BA%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/"},{"categories":["天池AI训练营"],"content":"学习思考与总结 通过此次得学习，我学到了逻辑回归的基本原理及其相关应用。逻辑回归有它的局限性，适合样本量较少的情况，而且精度不太高，但是用可解释性强。 因此，针对不同的项目，采用不同的方法很重要，若用神经网络处理鸢尾花数据集可能大材小用了，数据量大到一定程度，也许神经网络是个不错的选择。 ","date":"2021-06-01","objectID":"/%E5%9F%BA%E4%BA%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/:0:4","tags":["机器学习","逻辑回归","分类","“预测”"],"title":"基于逻辑回归的分类预测学习笔记","uri":"/%E5%9F%BA%E4%BA%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/"},{"categories":["LeetCode"],"content":"这篇文章展示了排序算法的总结.","date":"2021-06-01","objectID":"/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/","tags":["动态规划"],"title":"排序算法总结（多示例+讲解）","uri":"/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"},{"categories":["LeetCode"],"content":"动态规划功能强大，它能够解决子问题并使用这些答案来解决大问题。但 仅当每个子问题都是离散的，即不依赖于其他子问题时，动态规划才管用。 比如，想去以下地方旅游4天，假设将埃菲尔铁塔加入“背包”后，卢浮宫将 更“便宜”：只要1天时间，而不是1.5天。用动态规划对这种情况建模呢？ 这是没办法建模的，因为存在依赖关系。 ","date":"2021-06-01","objectID":"/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/:0:0","tags":["动态规划"],"title":"排序算法总结（多示例+讲解）","uri":"/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"},{"categories":["LeetCode"],"content":"一、冒泡排序 def bubble_sort(arr): for i in range(0, len(arr)): # 对每个元素 for j in range(1, len(arr)-i): # 最大的往上冒，冒完需要减1避免再次计算该值 if arr[j] \u003e arr[j+1]: # 此处，\"\u003e\"为大的数往上冒，\"\u003c\"为小的数往上冒 arr[j], arr[j+1] = arr[j+1], arr[j] # 交换位置 return a ","date":"2021-06-01","objectID":"/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/:0:1","tags":["动态规划"],"title":"排序算法总结（多示例+讲解）","uri":"/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"},{"categories":["LeetCode"],"content":"二、选择排序 def selection_sort(arr): for i in range(len(arr)-1): # 减1是为了第2个for的起始i+1 min_index = i for j in range(i+1, len(arr)): # 遍历后面的值，并记录最小值的索引 if arr[min_index] \u003e arr[j]: # 要是取最大值的索引，则改\"\u003e\"为\"\u003c\" min_index = j if i != min_index: # 如果已经改变了最小索引，则交换 arr[min_index], arr[i] = arr[i], arr[min_index] return arr ","date":"2021-06-01","objectID":"/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/:0:2","tags":["动态规划"],"title":"排序算法总结（多示例+讲解）","uri":"/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"},{"categories":["LeetCode"],"content":"三、插入排序 def insertion_sort(arr): for i in range(len(arr)): pre_index = i-1 # 获得前一个索引 current = arr[i] # 得到当前的值 while pre_index \u003e= 0 and arr[pre_index] \u003e current: # 每个arr[i]值与前面的比 arr[pre_index+1] = arr[pre_index] pre_index -= 1 arr[pre_index+1] = current # +1 可以理解为防止pre_index为 -1 return arr ","date":"2021-06-01","objectID":"/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/:0:3","tags":["动态规划"],"title":"排序算法总结（多示例+讲解）","uri":"/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"},{"categories":["LeetCode"],"content":"四、希尔排序 class Solution(object): def maxSubArray(self, nums): \"\"\" :type nums: List[int] :rtype: int \"\"\" for i in range(1, len(nums)): nums[i] += max(nums[i-1], 0) return max(nums) ","date":"2021-06-01","objectID":"/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/:0:4","tags":["动态规划"],"title":"排序算法总结（多示例+讲解）","uri":"/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"},{"categories":["LeetCode"],"content":"五、归并排序 class Solution(object): def maxSubArray(self, nums): \"\"\" :type nums: List[int] :rtype: int \"\"\" for i in range(1, len(nums)): nums[i] += max(nums[i-1], 0) return max(nums) ","date":"2021-06-01","objectID":"/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/:0:5","tags":["动态规划"],"title":"排序算法总结（多示例+讲解）","uri":"/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"},{"categories":["LeetCode"],"content":"六、快速排序 class Solution(object): def maxSubArray(self, nums): \"\"\" :type nums: List[int] :rtype: int \"\"\" for i in range(1, len(nums)): nums[i] += max(nums[i-1], 0) return max(nums) ","date":"2021-06-01","objectID":"/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/:0:6","tags":["动态规划"],"title":"排序算法总结（多示例+讲解）","uri":"/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"},{"categories":["LeetCode"],"content":"七、堆排序 class Solution(object): def maxSubArray(self, nums): \"\"\" :type nums: List[int] :rtype: int \"\"\" for i in range(1, len(nums)): nums[i] += max(nums[i-1], 0) return max(nums) ","date":"2021-06-01","objectID":"/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/:0:7","tags":["动态规划"],"title":"排序算法总结（多示例+讲解）","uri":"/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"},{"categories":["LeetCode"],"content":"八、计数排序 class Solution(object): def maxSubArray(self, nums): \"\"\" :type nums: List[int] :rtype: int \"\"\" for i in range(1, len(nums)): nums[i] += max(nums[i-1], 0) return max(nums) ","date":"2021-06-01","objectID":"/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/:0:8","tags":["动态规划"],"title":"排序算法总结（多示例+讲解）","uri":"/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"},{"categories":["LeetCode"],"content":"九、桶排序 class Solution(object): def maxSubArray(self, nums): \"\"\" :type nums: List[int] :rtype: int \"\"\" for i in range(1, len(nums)): nums[i] += max(nums[i-1], 0) return max(nums) ","date":"2021-06-01","objectID":"/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/:0:9","tags":["动态规划"],"title":"排序算法总结（多示例+讲解）","uri":"/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"},{"categories":["LeetCode"],"content":"十、基数排序 class Solution(object): def maxSubArray(self, nums): \"\"\" :type nums: List[int] :rtype: int \"\"\" for i in range(1, len(nums)): nums[i] += max(nums[i-1], 0) return max(nums) ","date":"2021-06-01","objectID":"/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/:0:10","tags":["动态规划"],"title":"排序算法总结（多示例+讲解）","uri":"/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"},{"categories":["天池AI训练营"],"content":"这篇文章展示了Matplotlib画各种图的总结.","date":"2021-05-31","objectID":"/matplotlib/","tags":["python画图"],"title":"Matplotlib画各种图的总结","uri":"/matplotlib/"},{"categories":["天池AI训练营"],"content":"数据可视化的图表种类繁多，各式各样，因此我们需要掌握如何在特定场景下使用特定的图表。 数据可视化是为业务目的服务的，好的可视化图表可以起到清晰准确反映业务结果的目的，在选 择使用何种图表时，通常我们需要首先考虑你想通过可视化阐述什么样的故事，受众是谁，以及打算如何分析结果。 关于如何利用数据创造出吸引人的、信息量大的、有说服力的故事，进而达到有效沟通的目的， 可以进一步阅读这本书《用数据讲故事》学习。 ","date":"2021-05-31","objectID":"/matplotlib/:0:0","tags":["python画图"],"title":"Matplotlib画各种图的总结","uri":"/matplotlib/"},{"categories":["天池AI训练营"],"content":"学习知识点概要 常见的场景分为5大类： 1）展示趋势变化（Evolution） 2）展示分布关系（Distribution） 3）展示相关关系（Correlation） 4）展示排序信息（Ranking） 5）展示组成关系（Part of a whole） ","date":"2021-05-31","objectID":"/matplotlib/:0:1","tags":["python画图"],"title":"Matplotlib画各种图的总结","uri":"/matplotlib/"},{"categories":["天池AI训练营"],"content":"学习内容 展示趋势变化的图 1.折线图（Line chart） import matplotlib.pyplot as plt import numpy as np import pandas as pd # 创建数据，分别对应X轴和Y轴，注意X轴要是有序排列的 df1=pd.DataFrame({'x1data': range(1,101), 'y1data': np.random.randn(100)}) df2=pd.DataFrame({'x2data': range(1,101), 'y2data': np.random.randn(100)}) df3=pd.DataFrame({'x3data': range(1,101), 'y3data': np.random.randn(100)}) # 设定式样，也可以选择其他的风格式样 seaborn-whitegrid plt.style.use('seaborn-darkgrid') # 创建调色板， 色卡用来控制每条线的颜色 palette = plt.get_cmap('Set1') # 绘图，设置画布大小 plt.figure(figsize=(15, 7)) # color： 控制线条颜色，red/skyblue/blue 等 # alpha： 控制线条透明度 # linestyle：控制线条式样，\"--\"， \"-\"， \"-.\"， \":\" 等 # linewidth：控制线条粗细大小 # num用调色板改变颜色 num = 0 plt.plot('x1data', 'y1data', data=df1, marker='', color=palette(num), linestyle=':', linewidth=2, alpha=0.9, label='data1') num += 1 plt.plot('x2data', 'y2data', data=df2, marker='', color=palette(num), linestyle='-.', linewidth=2, alpha=0.9, label='data2') num += 1 plt.plot('x3data', 'y3data', data=df3, marker='', color=palette(num), linestyle='--', linewidth=2, alpha=0.9, label='data3') # 画出label名在图中的位置 plt.legend(loc=1, ncol=3) plt.title(\"Multiple line plot\", loc='center', fontsize=12, fontweight=0, color='orange') plt.xlabel(\"xdata\") plt.ylabel(\"ydata\") plt.show() 画出的多折线图如下： 2.面积图（Area chart） import numpy as np import matplotlib.pyplot as plt # 创建数据 x = range(1, 15) y = [1, 4, 6, 7, 4, 9, 3, 2, 4, 1, 5, 4, 8, 7] # 设定式样，也可以选择其他的风格式样 seaborn-whitegrid plt.style.use('seaborn-darkgrid') plt.figure(figsize=(20,10)) # 绘图 # facecolor：控制填充颜色，red/skyblue/blue 等 # alpha： 控制填充透明度 # hatch: 控制阴影式样{'/', '\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'} plt.subplot(1,2,1) plt.fill_between(x, y, facecolor=\"skyblue\", alpha=0.4, hatch='/') plt.title('Area without line') # 在填充的基础上，添加一条折线，图形更加清晰 plt.subplot(1,2,2) plt.fill_between(x, y, facecolor=\"skyblue\", alpha=0.2, hatch='/') # 画线 plt.plot(x, y, color=\"skyblue\", alpha=0.6, linewidth=1.5) plt.title('Area with line') plt.show() 画出的面积图如下： 3.堆积面积图（Stacked area chart） 堆积面积图是基础面积图的一个延伸，它将多个类别的数据变化都显示在了一个图形中。 它有以下特点： 不同于多折线图的线条可能相互交叉，堆积面积图不会出现不同分类的数据点被遮盖、被隐藏的状况。每个类别都是都是堆积在下面类别面积图之上的。 堆积面积图与标准面积图不同，某一分类的值并非与纵坐标完全对应，而是通过折线之间的相对高度来表达。 堆积面积图不仅可以展示各类的发展趋势(面积图和折线图都能表示这个)， 可以表达总体的发展趋势和个种类间的关系，比如重要程度，大致占比等。 import numpy as np import matplotlib.pyplot as plt # 风格 plt.style.use('seaborn-darkgrid') # 画布大小 plt.figure(figsize=(10, 6)) # 定义x，y值 x = range(1, 6) y = [[1, 4, 6, 8, 9], [2, 2, 7, 10, 12], [2, 8, 5, 10, 6]] # 绘图，数据输入方式可以是一个X和多个Y，也可以将多列Y的数据合并成一个 plt.stackplot(x, y, colors=['blue', 'green', 'red'], labels=['A', 'B', 'C']) plt.legend(loc='upper left') plt.title('Stacked Area') plt.show() 画出的堆积面积图如下： 展示分布关系的图 1.小提琴图（Violin plot） 小提琴图是用来展示多组数据的分布状态以及概率密度。这种图表结合了箱形图和密度图的特征。 小提琴图的功能与箱型图类似。 它显示了一个（或多个）分类变量多个属性上的定量数据的分布， 从而可以比较这些分布。与箱形图不同，其中所有绘图单元都与实际数据点对应，小提琴图描述了 基础数据分布的核密度估计。 注意事项： 不适合展示只有很少组别的数据。 按照中位数排序能让数据看起来更直观。 小提琴图可以用seaborn包的violinplot方法实现。 具体代码可查看示例。 2.箱型图（Box plot） 箱形图（或盒须图）以一种利于变量之间比较或不同分类变量层次之间比较的方式来展 示定量数据的分布。矩形框显示数据集的上下四分位数，而矩形框中延伸出的线段（触 须）则用于显示其余数据的分布位置，剩下超过上下四分位间距的数据点则被视为“异常值”。 箱型图的基本作用如下： 数据异常值。箱形图为我们提供了识别异常值的一个标准：异常值被定义为小于Q1－1.5IQR或大于Q3+1.5IQR的值。 偏态和尾重。箱型图揭示了数据批分布偏态和尾重的部分信息，尽管它们不能给出偏态和尾重程度的精确度量，但可作为我们粗略估计的依据。 数据的形状。同一数轴上，几批数据的箱形图并行排列，几批数据的中位数、尾长、异常值、分布区间等形状信息便一目了然。在一批数据 中，哪几个数据点出类拔萃，哪些数据点表现不及一般，这些数据点放在同类其它群体中处于什么位置，可以通过比较各箱形图的异常值看出。 注意事项： 箱型图隐藏了每个分组的数据量信息，可以通过标注或箱子宽度来展现。 箱型图隐藏了背后的分布信息，当数据量较少时可以使用数据抖动(jitter),当数据量较大时可以使用小提琴图来展现。 箱型图可以直接使用seaborn的boxplot方法来实现。 具体代码可查看示例。 3.直方图（Histogram） import seaborn as sns import matplotlib.pyplot as plt from sklearn.datasets import load_boston boston=load_boston() y = boston['target'] # 获取图和坐标轴 f, axs = plt.subplots(3, 1, figsize=(10, 10)) # 计数标准直方图，axs[0,1,2]表示三个不同坐标的图 sns.histplot(y, stat='count', ax=axs[0]) # 归一化的直方图 sns.histplot(y, stat='probability', ax=axs[1]) # 在直方图上同时画出密度曲线, kde为计算核密度估计去平滑分布然后画出一条线 sns.histplot(y, stat='probability', kde=True, ax=axs[2]) plt.show() 画出的堆积面积图如下： 4.密度图（Density） import seaborn as sns import matplotlib.pyplot as plt import numpy as np # kdeplot()中的bw参数控制着估计值与真实数据之间的贴近程度 # 它与我们","date":"2021-05-31","objectID":"/matplotlib/:0:2","tags":["python画图"],"title":"Matplotlib画各种图的总结","uri":"/matplotlib/"},{"categories":["天池AI训练营"],"content":"学习思考与总结 本文的总结主要是简单汇总python可实现的不同的图，内容并不完整，更多内容需要查看官网相关示例才能知晓。 参考资料： matplotlib官网 seaborn官网 ","date":"2021-05-31","objectID":"/matplotlib/:0:3","tags":["python画图"],"title":"Matplotlib画各种图的总结","uri":"/matplotlib/"},{"categories":["大数据"],"content":"这篇文章介绍了Spark相关知识.","date":"2021-05-31","objectID":"/spark/","tags":["数据分析","分布式计算"],"title":"Spark","uri":"/spark/"},{"categories":["大数据"],"content":"Spark是专为大规模数据处理而设计的快速通用的计算引擎。 诞生于2009年，是加州大学伯克利分校RAD实验室的一个研究项目，最初基于Hadoop Mapreduce。 但是，由于Mapreduce在迭代式计算和交互式上低效，因此引入内存存储。 Spark包括多个紧密集成的组件： name: “featured-image” src: “featured-image.png” ","date":"2021-05-31","objectID":"/spark/:0:0","tags":["数据分析","分布式计算"],"title":"Spark","uri":"/spark/"},{"categories":["大数据"],"content":"Spark的组件 Spark core 基本功能：任务调度，内存管理，容错机制 内部定义了RDDs（Resilient distributed datasets, 弹性分布式数据集） 提供API创建和操作RDDs 在应用场景中，为其他组件提供底层的服务 Spark SQL Spark处理结构化数据的库，类似Hive SQL，Mysql 应用场景，企业用来做报表统计 Spark Streaming 实时数据流处理组件，类似storm 提供API操作实时流数据 应用场景，企业用来从Kafka（等消息队列中）接收数据做实时统计 Mlib 一个包含通用机器学习功能的包，Machine learning lib，包括分类、聚类、回归、模型评估和数据导入等 Mlib提供的方法都支持集群上的横向扩展（平时使用python是单机处理且有限的，而Mlib是集群的） 应用场景，机器学习 Graphx 处理图的库（如社交网络图），并进行图的并行计算 像Spark Steaming和Spark SQL一样，它也继承了RDDs API 提供了各种图操作和常用的图算法，例如PangeRank算法 应用场景，图计算 Cluster Managers 集群管理，Spark自带的一个集群管理是单独调度器 常见集群管理包括Hadoop YARN，Apache Mesos 紧密集成的优点 Spark底层优化了，基于Spark底层的组件也得到了相应的优化 紧密集成，节省了各个组件组合使用时的部署，测试等时间 向Spark增加新的组件时，其它组件可立刻享用新组件的功能 ","date":"2021-05-31","objectID":"/spark/:0:1","tags":["数据分析","分布式计算"],"title":"Spark","uri":"/spark/"},{"categories":["大数据"],"content":"Spark与Hadoop比较 Spark应用场景：时效性要求高（因为基于内存）、机器学习领域 Spark不具有HDFS（分布式文件系统）的存储能力，要借助HDFS等工具来持久化数据 Hadoop应用场景：离线处理、对时效性要求不高 ","date":"2021-05-31","objectID":"/spark/:0:2","tags":["数据分析","分布式计算"],"title":"Spark","uri":"/spark/"},{"categories":["大数据"],"content":"安装 Spark安装 1. Spark下载，安装 1.1 下载地址：http://spark.apache.org/downloads.html， 版本需匹配：Spark 1.6.2 - Scala 2.10 或 Spark 2.0.0 - Scala 2.11 1.2 解压 2. Spark Shell操作 2.1 目录： bin包含用来和Spark交互的可执行文件，如Spark shell core，steaming，python，…包含主要组件的源代码 examples包含一些单机Spark job，可以用来研究和运行的例子 2.2 Spark的Shell 能够处理分布在集群上的数据 Spark把数据加载到节点的内存中，因此分布式处理可在秒级完成 快速迭代式计算，实时查询、分析一般能够在shells中完成 Spark提供了Python shells和Scala shells 3. Spark开发环境搭建 3.1 IntelliJ IDEA的下载、安装： 下载地址：https://www.jetbrains.com/idea/ 3.2 插件安装 在IntelliJ IDEA开发环境中安装即可 3.3 搭建开发环境常遇到的问题 网络问题 版本匹配问题：Scala2.10.5，Jdk1.8，Spark1.6.2，Sbt0.13.8 Scala安装 Spark下载，安装 下载地址：http://www.scala-lang.org/download/2.10.5.html 版本需匹配：Spark 1.6.2 - Scala 2.10 或 Spark 2.0.0 - Scala 2.11 Scala基础知识 在Scala创建变量时，必须使用val或var val，变量值不可修改，一旦分配不能重新指向别的值 var，分配后，可以指向类型相同的值 ","date":"2021-05-31","objectID":"/spark/:0:3","tags":["数据分析","分布式计算"],"title":"Spark","uri":"/spark/"},{"categories":["大数据"],"content":"RDDs介绍 通过SparkContext对象访问Spark, SparkContext对象代表和一个集群的连接, 在Shell中SparkContext自动创建好了，就是sc RDDs并行的分布在整个集群中，如将500G的一个执行文件划分成5个100G的文件到不同的机器并行 RDDs是Spark分发数据和计算的基础抽象类 一个RDD是一个不可改变的分布式集合对象 Spark中，所有的计算都是通过RDDs的创建，转换，操作完成的 一个RDD内部由许多partitions（分片）组成， 分片： 每个分片包括一部分数据，partitions可在集群不同节点上计算 分片是Spark并行处理的单元，Spark顺序的，并行的处理分片 RDDs创建方法： 把一个存在的集合传给SparkContext的parallelize方法，测试用 val rdd = sc.parallelize(Array(1,2,2,4),4) 第一个参数：待并行化处理的集合，第二个参数：分区个数 加载外部数据集： val = rddText = sc。textFile(“helloSpark.txt”) ","date":"2021-05-31","objectID":"/spark/:0:4","tags":["数据分析","分布式计算"],"title":"Spark","uri":"/spark/"},{"categories":["大数据"],"content":"这篇文章介绍了Spark相关知识.","date":"2021-05-31","objectID":"/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/","tags":["数据分析","推荐系统"],"title":"推荐系统介绍","uri":"/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"},{"categories":["大数据"],"content":"Spark是专为大规模数据处理而设计的快速通用的计算引擎。 诞生于2009年，是加州大学伯克利分校RAD实验室的一个研究项目，最初基于Hadoop Mapreduce。 但是，由于Mapreduce在迭代式计算和交互式上低效，因此引入内存存储。 Spark包括多个紧密集成的组件： name: “featured-image” src: “featured-image.png” ","date":"2021-05-31","objectID":"/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/:0:0","tags":["数据分析","推荐系统"],"title":"推荐系统介绍","uri":"/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"},{"categories":["大数据"],"content":"推荐系统应用 个性化音乐 基本功能：任务调度，内存管理，容错机制 内部定义了RDDs（Resilient distributed datasets, 弹性分布式数据集） 提供API创建和操作RDDs 在应用场景中，为其他组件提供底层的服务 电子商务 Spark处理结构化数据的库，类似Hive SQL，Mysql 应用场景，企业用来做报表统计 电影视频 实时数据流处理组件，类似storm 提供API操作实时流数据 应用场景，企业用来从Kafka（等消息队列中）接收数据做实时统计 社交网络 一个包含通用机器学习功能的包，Machine learning lib，包括分类、聚类、回归、模型评估和数据导入等 Mlib提供的方法都支持集群上的横向扩展（平时使用python是单机处理且有限的，而Mlib是集群的） 应用场景，机器学习 个性化阅读 处理图的库（如社交网络图），并进行图的并行计算 像Spark Steaming和Spark SQL一样，它也继承了RDDs API 提供了各种图操作和常用的图算法，例如PangeRank算法 应用场景，图计算 位置服务 集群管理，Spark自带的一个集群管理是单独调度器 常见集群管理包括Hadoop YARN，Apache Mesos 个性化邮件 Spark底层优化了，基于Spark底层的组件也得到了相应的优化 紧密集成，节省了各个组件组合使用时的部署，测试等时间 向Spark增加新的组件时，其它组件可立刻享用新组件的功能 ","date":"2021-05-31","objectID":"/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/:0:1","tags":["数据分析","推荐系统"],"title":"推荐系统介绍","uri":"/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"},{"categories":["大数据"],"content":"个性化广告 Spark应用场景：时效性要求高（因为基于内存）、机器学习领域 Spark不具有HDFS（分布式文件系统）的存储能力，要借助HDFS等工具来持久化数据 Hadoop应用场景：离线处理、对时效性要求不高 ","date":"2021-05-31","objectID":"/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/:0:2","tags":["数据分析","推荐系统"],"title":"推荐系统介绍","uri":"/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"},{"categories":["大数据"],"content":"个性化旅游 Spark应用场景：时效性要求高（因为基于内存）、机器学习领域 Spark不具有HDFS（分布式文件系统）的存储能力，要借助HDFS等工具来持久化数据 Hadoop应用场景：离线处理、对时效性要求不高 ","date":"2021-05-31","objectID":"/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/:0:3","tags":["数据分析","推荐系统"],"title":"推荐系统介绍","uri":"/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"},{"categories":["大数据"],"content":"证券、投资 Spark应用场景：时效性要求高（因为基于内存）、机器学习领域 Spark不具有HDFS（分布式文件系统）的存储能力，要借助HDFS等工具来持久化数据 Hadoop应用场景：离线处理、对时效性要求不高 ","date":"2021-05-31","objectID":"/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/:0:4","tags":["数据分析","推荐系统"],"title":"推荐系统介绍","uri":"/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"},{"categories":["大数据"],"content":"推荐系统分类 根据实时性分类 1. 离线推荐 1.1 下载地址：http://spark.apache.org/downloads.html， 版本需匹配：Spark 1.6.2 - Scala 2.10 或 Spark 2.0.0 - Scala 2.11 1.2 解压 2. 实时推荐 2.1 目录： 根据推荐是否个性化分类 1. 基于统计的推荐 1.1 下载地址：http://spark.apache.org/downloads.html， 版本需匹配：Spark 1.6.2 - Scala 2.10 或 Spark 2.0.0 - Scala 2.11 1.2 解压 2. 个性化推荐 2.1 目录： 根据推荐原则分类 1. 基于相似度的推荐 1.1 下载地址：http://spark.apache.org/downloads.html， 版本需匹配：Spark 1.6.2 - Scala 2.10 或 Spark 2.0.0 - Scala 2.11 1.2 解压 2. 基于知识的推荐 2.1 目录： 3. 基于模型的推荐 3.1 目录： 根据数据源分类 1. 基于人口统计学的推荐 1.1 下载地址：http://spark.apache.org/downloads.html， 版本需匹配：Spark 1.6.2 - Scala 2.10 或 Spark 2.0.0 - Scala 2.11 1.2 解压 2. 基于内容的推荐 2.1 目录： 3. 基于协同过滤的推荐 3.1 目录： ","date":"2021-05-31","objectID":"/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/:0:5","tags":["数据分析","推荐系统"],"title":"推荐系统介绍","uri":"/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"},{"categories":["机器学习"],"content":"这篇文章展示了提升树模型.","date":"2021-05-31","objectID":"/%E6%8F%90%E5%8D%87%E6%A0%91/","tags":["树","机器学习"],"title":"提升树模型","uri":"/%E6%8F%90%E5%8D%87%E6%A0%91/"},{"categories":["机器学习"],"content":"回归问题的提升树方法 算法1： 输入：训练数据集$ T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}, x_i \\in X \\subseteq R^n, y_i \\in Y \\subseteq R $ 输出：提升树$ f_m(x) $ (1) 初始化$ f_0(x)=0 $ (2) 对$ m=1,2,…,M $ (2.1) 按式(1)计算残差$ r_{mi}=y_i-f_{m-1}(x_i) $ (2.2) 拟合残差$r_{mi}$学习一个回归树，得到$T(x;\\Theta_m)$ (2.3) 更新$ f_m(x)=f_{m-1}(x)+T(x;\\Theta_m) $ (3) 得到回归问题的提升树$ f_M(x)=\\displaystyle\\sum_{m=1}^M T(x;\\Theta_m) $ 例子： $x_i$ 1 2 3 4 5 6 7 8 9 $y_i$ 5.56 5.70 5.91 6.40 6.80 7.05 8.90 8.70 9.00 求$ T_1(x) $ 2.1 训练数据的某一个划分点s: $ R_1= \\lbrace x|x \\leq s \\rbrace, R_2= \\lbrace x|x \\gt s \\rbrace $ 在集合$R_1,R_2$中，计算$ c_1,c_2 $为 $ c_1=\\frac{1}{N_1}\\displaystyle\\sum_{x_i \\in R_1} y_i$, $c_2=\\frac{1}{N_2}\\displaystyle\\sum_{x_i \\in R_2} y_i $, 这里的$N_1,N_2$分别为$R_1,R_2$的样本点数。 设定切分点s，根据训练集样本可知，考虑如下切分点（也可用本身数据值作划分）： 1.5，2.5，3.5，4.5，5.5，6.5，7.5，8.5，9.5 根据优化问题的公式：$m(s)=\\min_s[\\min_{c_1}\\displaystyle\\sum_{x_i \\in R_1}(y_i-c_1)^2 + min_{c_2}\\displaystyle\\sum_{x_i \\in R_2}(y_i-c_2)^2]$ 求出相应的$R_1,R_2,c_1,c_2$及$ m(s) $ 当s=1.5时，$ R_1=\\lbrace 1 \\rbrace , R_2=\\lbrace 2,3,…,10 \\rbrace, c_1=5.56, c_2=7.50, m(s)=0+15.72=15.72 $ 将s和$m(s)$的计算结果列出如下表1： $$表1 计算数据表$$ $s$ 1.5 2.5 3.5 4.5 5.5 6.5 7.5 8.5 9.5 $m(s)$ 15.72 12.07 8.36 5.78 3.91 1.93 8.01 11.73 15.74 由表1可知，当s=6.5时$m(s)$达到最大值，此时 $ R_1=\\lbrace 1,2,…,6 \\rbrace , R_2=\\lbrace 7,8,9,10 \\rbrace, c_1=6.24, c_2=8.91 $， 所以回归树$ T_1(x) $为 $$ T_1(x) = \\begin{cases} 6.24, \u0026 x \\lt 6.5 \\\\ 8.91, \u0026 x \\ge 6.5 \\end{cases} $$ $$ f_1(x)=T_1(x) $$ 用$ f_1(x) $拟合数据的残差（输出值和真实值之差）见表2所示，表中$ r_{2i}=y_i-f_1(x_i),i=1,2,…,10 $ $$ 表2 数据残差表 $$ $x_i$ 1 2 3 4 5 6 7 8 9 $r_{2i}$ -0.68 -0.54 -0.33 0.16 0.56 0.81 -0.01 -0.21 0.09 此时，用$ f_1(x) $拟合训练数据的平方损失误差为：$$ L(y,f_1(x))=\\displaystyle\\sum_{i=1}^{10}(y_i-f_1(x_i))^2=1.93 $$ 求$T_2(x)$ 同理，求$T_2(x)$和求$T_1(x)$的方法是一样的，只是$ f_1(x) $拟合数据的残差，$ f_2(x) $拟合的数据为表2的残差，求得 $$ T_2(x) = \\begin{cases} -0.52, \u0026 x \\lt 3.5 \\\\ 0.22, \u0026 x \\ge 3.5 \\end{cases} $$ $$ f_2(x)=f_1(x)+T_2(x)= \\begin{cases} 5.72, \u0026 x \\lt 3.5 \\\\ 6.46, \u0026 x \\ge 3.5 \\cup x \\lt 6.5 \\\\ 9.13, \u0026 x \\ge 6.5 \\end{cases} $$ 此时，用$ f_2(x) $拟合表的平方损失误差为：$$ L(y,f_2(x))=\\displaystyle\\sum_{i=1}^{10}(y_i-f_2(x_i))^2=0.79 $$ 继续求得： $$ T_3(x) = \\begin{cases} 0.15, \u0026 x \\lt 6.5 \\\\ -0.22, \u0026 x \\ge 6.5 \\end{cases}, L(y,f_3(x))=0.47 $$ $$ T_4(x) = \\begin{cases} -0.16, \u0026 x \\lt 4.5 \\\\ 0.11, \u0026 x \\ge 4.5 \\end{cases}, L(y,f_4(x))=0.30 $$ $$ T_5(x) = \\begin{cases} 0.07, \u0026 x \\lt 6.5 \\\\ -0.11, \u0026 x \\ge 6.5 \\end{cases}, L(y,f_5(x))=0.23 $$ $$ T_6(x) = \\begin{cases} -0.15, \u0026 x \\lt 2.5 \\\\ 0.04, \u0026 x \\ge 2.5 \\end{cases}, f_6(x)=f_5(x)+T_6(x)= \\begin{cases} 5.63, \u0026 x \\lt 2.5 \\\\ 5.82, \u0026 x \\ge 2.5 \\cup x \\lt 3.5 \\\\ 6.56, \u0026 x \\ge 3.5 \\cup x \\lt 4.5 \\\\ 6.83, \u0026 x \\ge 4.5 \\cup x \\lt 5.5 \\\\ 8.95, \u0026 x \\ge 6.5 \\end{cases} $$ 此时，用$ f_6(x) $拟合表的平方损失误差为：$$ L(y,f_6(x))=\\displaystyle\\sum_{i=1}^{10}(y_i-f_6(x_i))^2=0.17 $$ 假设当前的误差0.17已经满足误差要求，则$f(x)=f_6(x)$为所求的提升树。 ","date":"2021-05-31","objectID":"/%E6%8F%90%E5%8D%87%E6%A0%91/:0:1","tags":["树","机器学习"],"title":"提升树模型","uri":"/%E6%8F%90%E5%8D%87%E6%A0%91/"},{"categories":["机器学习"],"content":"梯度提升 当损失函数是平方损失和指数损失函数时，每一步的优化是很简单的，但是对于一般的损失函数而言， 往往每一步优化没那么容易。因此，有研究人员提出了梯度提升算法，这是利用最速下降法的近似方 法，关键是利用损失函数的负梯度在当前模型的值$$ -[\\frac{\\partial L(y,f(x_i))}{\\partial f(x_i)}]_{f(x)=f_m-1(x)} $$作为回归问题 提升树算法中的残差的近似值，拟合一回归树。 算法2： 输入：训练数据集$ T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}, x_i \\in X \\subseteq R^n, y_i \\in Y \\subseteq R $ 输出：回归树$ f(x) $ (1) 初始化$ f_0(x)=argmin_c \\displaystyle\\sum_{i=1}^N L(y_i,c) $ (2) 对$ m=1,2,…,M $ (2.1) 对$ i=1,2,…,N $，计算 $$ r_{mi}=-[\\frac{\\partial L(y,f(x_i))}{\\partial f(x_i)}]_{f(x)=f_m-1(x)} $$ (2.2) 对$r_{mi}$拟合一个回归树，得到第m棵树的叶结点区域$R_mj,j=1,2,…,J$ (2.3) 对$j=1,2,…,J$，计算$ c_{mi}=argmin_c \\displaystyle\\sum_{xi \\in R_mj} L(y_i,f_{m-1}(x_i)+c) $ (2.4) 更新$ f_m(x)=f{m-1}(x)+\\displaystyle\\sum_{j=1}^J c_{mj}I(x \\in R_{mj}) $ (3) 得到回归问题$ f(x)=f_M(x)=\\displaystyle\\sum_{m=1}^M \\displaystyle\\sum_{j=1}^J c_{mj}I(x \\in R_{mj}) $ 说明： 第（1）步初始化，估计使损失函数最小化的常数值 第（2.1）步计算损失函数的负梯度在当前模型的值，将它作为残差的估计。注：对于平方损失函数，为残差；对于一般的损失函数，为残差近似值 第（2.2）步估计回归树叶结点区域，以拟合残差的近似值 第（2.3）步利用线性搜索估计叶结点区域的值，使损失函数最小化 第（2.4）步更新回归树 第（3）步得到输出的最终模型$f(x)$ 代码实现： ","date":"2021-05-31","objectID":"/%E6%8F%90%E5%8D%87%E6%A0%91/:0:2","tags":["树","机器学习"],"title":"提升树模型","uri":"/%E6%8F%90%E5%8D%87%E6%A0%91/"},{"categories":["python"],"content":"这篇文章展示了基本的 python 数据结构函数的操作.","date":"2021-05-28","objectID":"/python%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%93%8D%E4%BD%9C/","tags":["python","数据结构"],"title":"python数据结构栈和队列的相关方法","uri":"/python%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%93%8D%E4%BD%9C/"},{"categories":["python"],"content":"s = stack() stack()函数方法如下： 栈操作 栈内容 返回值 s.is_empty() [] True s.push(4) [4] s.push(‘dog’) [4,’dog’] s.peek() [4,’dog’] ‘dog’ s.push(True) [4,’dog’,True] s.size() [4,’dog’,True] 3 s.is_empty() [4,’dog’,True] False s.push(8.4) [4,’dog’,True,8.4] s.pop() [4,’dog’,True] 8.4 s.pop() [4,’dog’] True s.size() [4,’dog’] 2 q = Queue() Queue()函数方法如下： 队列操作 队列内容 返回值 q=Queue() [] Queue 对象 q.isEmpty() [] True q.enqueue(4) [4] q.enqueue(‘dog’) [’dog’, 4] q.enqueue(True) [True,‘dog’,4] q.size() [True,‘dog’,4] 3 q.isEmpty() [True,‘dog’,4] False q.enqueue(8.4) [8.4, True,‘dog’,4] q.dequeue() [8.4,True,‘dog’] 4 q.dequeue() [8.4,True] ‘dog’ q.size() [8.4,True] 2 d = Deque() Deque()函数方法如下： 双端队列操作 双端队列内容 返回值 d=Deque() [] Deque 对象 d.isEmpty() [] True d.addRear(4) [4] d.addRear(‘dog’) [’dog’, 4] d.addFront(‘cat’) [’dog’, 4, ‘cat’] d.addFront(True) [‘dog’,4, ‘cat’, True] d.size() [‘dog’, 4, ‘cat’, True] 4 d.isEmpty() [‘dog’, 4, ‘cat’, True] False d.addRear(8.4) [8.4, ‘dog’, 4, ‘cat’,True] d.removeRear() [‘dog’, 4, ‘cat’, True] 8.4 d.removeFront() [‘dog’, 4, ‘cat’] True 😄 ","date":"2021-05-28","objectID":"/python%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%93%8D%E4%BD%9C/:0:0","tags":["python","数据结构"],"title":"python数据结构栈和队列的相关方法","uri":"/python%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%93%8D%E4%BD%9C/"},{"categories":null,"content":"Introduction  This blog was built for recording some knowledges.  If we have the awareness of life-long learning, I think we must live better in the world.  In the way of whole life, we need to be more powerful than the former self instead of always comparing with others. ","date":"2021-05-26","objectID":"/about/:0:1","tags":null,"title":"Hi! Welcome!","uri":"/about/"}]